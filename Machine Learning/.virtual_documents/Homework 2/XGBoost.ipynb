




















%%capture
pip install xgboost


%%capture
pip install kagglehub





import numpy as np
import pandas as pd
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import xgboost as xgb
import matplotlib.pyplot as plt
import kagglehub
import os
from sklearn.model_selection import GridSearchCV
import seaborn as sns





csv_path = kagglehub.dataset_download("camnugent/california-housing-prices")





np.random.seed(42)





data = pd.read_csv(os.path.join(csv_path, "housing.csv"))

print(data.info())
data.head()





# Handling missing values (if any)
data.dropna(inplace=True)
data = pd.get_dummies(data, columns=["ocean_proximity"], drop_first=True)
# Separating features and target
X = data.drop("median_house_value", axis=1)
y = data["median_house_value"]

# Splitting the data into training and test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print(f"Training features: {X_train.shape}")
print(f"Test features: {X_test.shape}")





# Set up the XGBoost model
xgb_model = xgb.XGBRegressor(
    max_depth=6,
    learning_rate=0.1,
    n_estimators=100,
    objective='reg:squarederror',
    reg_lambda=1,
    reg_alpha=0,
    gamma=1,
    random_state=42
)

# Train the model
xgb_model.fit(X_train, y_train)





y_pred = xgb_model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.4f}")





xgb.plot_importance(xgb_model, importance_type="weight", max_num_features=10)
plt.title("Feature Importance by Weight")
plt.show()





param_grid = {
    'max_depth': [3, 6, 9],
    'learning_rate': [0.01, 0.1, 0.3],
}

grid_search = GridSearchCV(
    estimator=xgb.XGBRegressor(n_estimators=100, objective='reg:squarederror', random_state=42),
    param_grid=param_grid,
    scoring='r2',
    cv=3
)
grid_search.fit(X_train, y_train)

print("Best parameters:", grid_search.best_params_)
print("Best R^2 score:", grid_search.best_score_)








plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred, alpha=0.5, edgecolor='k')
plt.xlabel("Actual Values")
plt.ylabel("Predicted Values")
plt.title("Actual vs. Predicted Median House Values")
plt.grid(True)
plt.show()





residuals = y_test - y_pred

plt.figure(figsize=(10, 6))
plt.scatter(y_pred, residuals, alpha=0.5, edgecolor='k')
plt.axhline(y=0, color='r', linestyle='--', linewidth=1)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Residuals vs. Predicted Values")
plt.grid(True)
plt.show()





plt.figure(figsize=(12, 10))
sns.heatmap(X.corr(), cmap='coolwarm', annot=False, fmt=".2f", cbar=True)
plt.title("Feature Correlation Heatmap")
plt.show()






