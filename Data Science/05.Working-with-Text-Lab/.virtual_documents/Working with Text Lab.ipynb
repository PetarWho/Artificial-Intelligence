import pandas as pd
import string
from nltk.tokenize import word_tokenize
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.stem import WordNetLemmatizer
from collections import Counter
import matplotlib.pyplot as plt
import re
from sklearn.feature_extraction.text import CountVectorizer
from transformers import AutoTokenizer, AutoModel
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import AgglomerativeClustering
import torch











df = pd.read_csv('data/European Restaurant Reviews.csv')

print(df.head())
print(df.info())
print(df.describe(include='all'))


num_observations = df.shape[0]
print(f'Number of observations: {num_observations}')


country_counts = df['Country'].value_counts()
most_represented_country = country_counts.idxmax()
most_represented_country_count = country_counts.max()
print(f'Most represented country: {most_represented_country} with {most_represented_country_count} reviews')


df['Review Date'] = pd.to_datetime(df['Review Date'], errors='coerce')

min_date = df['Review Date'].min()
max_date = df['Review Date'].max()
print(f'Time range: {min_date} to {max_date}')


restaurant_counts = df['Restaurant Name'].value_counts()
balanced_restaurants = restaurant_counts.value_counts().min() == restaurant_counts.value_counts().max()
print(f'The dataset is{"not" if not balanced_restaurants else ""} balanced in terms of restaurants')





def word_count(text):
    if pd.isna(text):
        return 0
    return len(text.split())

df['Word Count'] = df['Review'].apply(word_count)

average_word_count = df.groupby('Sentiment')['Word Count'].mean()
print(average_word_count)

print(f'Average word count for Positive reviews: {average_word_count["Positive"]}')
print(f'Average word count for Negative reviews: {average_word_count["Negative"]}')











df['Review'] = df['Review'].str.lower()





def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

df['Review'] = df['Review'].apply(remove_punctuation)





def remove_numbers(text):
    return ''.join([i for i in text if not i.isdigit()])

df['Review'] = df['Review'].apply(remove_numbers)





df['Review'] = df['Review'].str.strip()
df['Review'] = df['Review'].str.replace('\s+', ' ', regex=True)





nltk.download('punkt')

df['Tokens'] = df['Review'].apply(word_tokenize)





nltk.download('stopwords')

stop_words = set(stopwords.words('english'))

def remove_stopwords(tokens):
    return [word for word in tokens if word not in stop_words]

df['Tokens'] = df['Tokens'].apply(remove_stopwords)





stemmer = PorterStemmer()

def stem_tokens(tokens):
    return [stemmer.stem(token) for token in tokens]

df['Tokens'] = df['Tokens'].apply(stem_tokens)





nltk.download('wordnet')
lemmatizer = WordNetLemmatizer()

def lemmatize_tokens(tokens):
    return [lemmatizer.lemmatize(token) for token in tokens]

df['Tokens'] = df['Tokens'].apply(lemmatize_tokens)





def handle_negations(tokens):
    negations = {'not', 'no', 'never'}
    result = []
    skip = False
    for i, token in enumerate(tokens):
        if token in negations:
            skip = True
            continue
        if skip:
            result.append(token + '_NEG')
            skip = False
        else:
            result.append(token)
    return result

df['Tokens'] = df['Tokens'].apply(handle_negations)





df['Cleaned_Review'] = df['Tokens'].apply(lambda tokens: ' '.join(tokens))

df.to_csv('data/preprocessed_reviews.csv', index=False)








df['Review'] = df['Review'].str.lower()
df['Review'] = df['Review'].str.replace('[^\w\s]', '', regex=True)
df['Review'] = df['Review'].str.replace('\d+', '', regex=True)
df['Tokens'] = df['Review'].apply(lambda x: x.split())  # Simple tokenization

positive_reviews = df[df['Sentiment'] == 'Positive']['Tokens']
negative_reviews = df[df['Sentiment'] == 'Negative']['Tokens']

def get_top_words(tokens_list, top_n=10):
    all_tokens = [token for tokens in tokens_list for token in tokens]
    word_counts = Counter(all_tokens)
    return word_counts.most_common(top_n)

top_positive_words = get_top_words(positive_reviews)
top_negative_words = get_top_words(negative_reviews)

print("Top 10 Words in Positive Reviews:")
for word, count in top_positive_words:
    print(f"{word}: {count}")

print("\nTop 10 Words in Negative Reviews:")
for word, count in top_negative_words:
    print(f"{word}: {count}")

def plot_top_words(top_words, title):
    words, counts = zip(*top_words)
    plt.figure(figsize=(10, 6))
    plt.bar(words, counts, color='skyblue')
    plt.title(title)
    plt.xlabel('Words')
    plt.ylabel('Frequency')
    plt.xticks(rotation=45)
    plt.show()

plot_top_words(top_positive_words, 'Top 10 Words in Positive Reviews')
plot_top_words(top_negative_words, 'Top 10 Words in Negative Reviews')





def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    text = re.sub(r'\d+', '', text)  # Remove numbers
    return text

df['Cleaned_Title'] = df['Review Title'].apply(preprocess_text)
df['Title_Tokens'] = df['Cleaned_Title'].apply(lambda x: word_tokenize(x))

def remove_stop_words(tokens):
    return [word for word in tokens if word not in stop_words]

df['Title_Tokens_No_Stopwords'] = df['Title_Tokens'].apply(remove_stop_words)

def get_top_words(tokens_list, top_n=10):
    all_tokens = [token for tokens in tokens_list for token in tokens]
    word_counts = Counter(all_tokens)
    return word_counts.most_common(top_n)

positive_titles = df[df['Sentiment'] == 'Positive']['Title_Tokens']
negative_titles = df[df['Sentiment'] == 'Negative']['Title_Tokens']

positive_titles_no_stopwords = df[df['Sentiment'] == 'Positive']['Title_Tokens_No_Stopwords']
negative_titles_no_stopwords = df[df['Sentiment'] == 'Negative']['Title_Tokens_No_Stopwords']

top_positive_words = get_top_words(positive_titles)
top_negative_words = get_top_words(negative_titles)

top_positive_words_no_stopwords = get_top_words(positive_titles_no_stopwords)
top_negative_words_no_stopwords = get_top_words(negative_titles_no_stopwords)

print("Top 10 Words in Positive Titles (With Stop Words):")
for word, count in top_positive_words:
    print(f"{word}: {count}")

print("\nTop 10 Words in Negative Titles (With Stop Words):")
for word, count in top_negative_words:
    print(f"{word}: {count}")

print("\nTop 10 Words in Positive Titles (Without Stop Words):")
for word, count in top_positive_words_no_stopwords:
    print(f"{word}: {count}")

print("\nTop 10 Words in Negative Titles (Without Stop Words):")
for word, count in top_negative_words_no_stopwords:
    print(f"{word}: {count}")





def preprocess_text(text):
    text = text.lower()
    text = re.sub(r'[^\w\s]', '', text) 
    text = re.sub(r'\d+', '', text) 
    return text

df['Cleaned_Title'] = df['Review Title'].apply(preprocess_text)
df['Cleaned_Review'] = df['Review'].apply(preprocess_text)
df['Title_Content'] = df['Cleaned_Title'] + ' ' + df['Cleaned_Review']

vectorizer_titles = CountVectorizer(
    ngram_range=(1, 2),
    stop_words='english',
    max_features=500,
    min_df=1,
    max_df=0.7
)

X_titles = vectorizer_titles.fit_transform(df['Cleaned_Title'])

print("Top features in Title Vectorizer:")
top_features_titles = vectorizer_titles.get_feature_names_out()
print(top_features_titles[:20])  

vectorizer_contents = CountVectorizer(
    ngram_range=(1, 3),
    stop_words='english',
    max_features=1000,
    min_df=5,
    max_df=0.5
)

X_contents = vectorizer_contents.fit_transform(df['Cleaned_Review'])

print("\nTop features in Content Vectorizer:")
top_features_contents = vectorizer_contents.get_feature_names_out()
print(top_features_contents[:20])  

vectorizer_combined = CountVectorizer(
    ngram_range=(1, 2),
    stop_words='english',
    max_features=1000,
    min_df=5,
    max_df=0.5
)

X_combined = vectorizer_combined.fit_transform(df['Title_Content'])

print("\nTop features in Combined Vectorizer:")
top_features_combined = vectorizer_combined.get_feature_names_out()
print(top_features_combined[:20])  












