import os
from PIL import Image
import numpy as np
from collections import defaultdict
import imagehash
import tensorflow as tf
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.applications.resnet50 import preprocess_input, decode_predictions
from tensorflow.keras.preprocessing import image
from sklearn.metrics import confusion_matrix, classification_report











dataset_path = 'data/'

cat_types = [
    'african-wildcat', 'blackfoot-cat', 'chinese-mountain-cat', 
    'domestic-cat', 'european-wildcat', 'jungle-cat', 'sand-cat'
]

# Count the number of cat types
num_cat_types = len(cat_types)
print(f"Number of cat types: {num_cat_types}")


# Count the number of images for each cat type
image_counts = defaultdict(int)
for cat_type in cat_types:
    cat_folder = os.path.join(dataset_path, cat_type)
    if os.path.isdir(cat_folder):
        num_images = len([f for f in os.listdir(cat_folder) if os.path.isfile(os.path.join(cat_folder, f))])
        image_counts[cat_type] = num_images

print("\nNumber of images for each cat type:")
for cat_type, count in image_counts.items():
    print(f"{cat_type}: {count} images")


# Determine typical image size and check for outliers
image_sizes = []
for cat_type in cat_types:
    cat_folder = os.path.join(dataset_path, cat_type)
    for img_file in os.listdir(cat_folder):
        img_path = os.path.join(cat_folder, img_file)
        if os.path.isfile(img_path):
            try:
                with Image.open(img_path) as img:
                    image_sizes.append(img.size)
            except:
                print(f"Skipping file {img_path}, unable to open as an image")


image_sizes = np.array(image_sizes)

if len(image_sizes) > 0:
    mean_size = np.mean(image_sizes, axis=0)
    std_size = np.std(image_sizes, axis=0)

    print(f"\nTypical image size (mean): {mean_size}")
    print(f"Standard deviation of image size: {std_size}")

    size_threshold = mean_size + 2 * std_size
    outliers = image_sizes[np.any(image_sizes > size_threshold, axis=1)]

    print(f"Number of outliers: {len(outliers)}")
else:
    print("No images were processed, so size analysis cannot be performed.")





dataset_path = 'data/'

cat_types = [
    'african-wildcat', 'blackfoot-cat', 'chinese-mountain-cat', 
    'domestic-cat', 'european-wildcat', 'jungle-cat', 'sand-cat'
]

image_hashes = {}
duplicates = defaultdict(list)

# Function to compute the perceptual hash of an image
def compute_hash(image_path):
    try:
        with Image.open(image_path) as img:
            hash_value = imagehash.phash(img)
        return hash_value
    except Exception as e:
        print(f"Error processing {image_path}: {e}")
        return None


# Process each image
for cat_type in cat_types:
    cat_folder = os.path.join(dataset_path, cat_type)
    if os.path.isdir(cat_folder):
        for img_file in os.listdir(cat_folder):
            img_path = os.path.join(cat_folder, img_file)
            if os.path.isfile(img_path):
                img_hash = compute_hash(img_path)
                if img_hash:
                    if img_hash in image_hashes:
                        duplicates[img_hash].append(img_path)
                    else:
                        image_hashes[img_hash] = img_path


print("Duplicate images:")
for img_hash, paths in duplicates.items():
    print(f"Hash: {img_hash}")
    for path in paths:
        print(f"  {path}")

for paths in duplicates.values():
    for path in paths[1:]:  # Keep the first occurrence and remove the rest
        os.remove(path)
        print(f"Removed: {path}")

print("Duplicate removal completed.")





model = ResNet50(weights='imagenet')

num_layers = len(model.layers)
print(f"Total number of layers: {num_layers}")

num_params = model.count_params()
print(f"Total number of parameters: {num_params}")

model.summary()





def load_and_preprocess_image(img_path, target_size=(224, 224)):
    img = image.load_img(img_path, target_size=target_size)
    img_array = image.img_to_array(img)
    img_array = np.expand_dims(img_array, axis=0)
    img_array = preprocess_input(img_array)
    return img_array

data_folder = 'data/'

subfolders = ['african-wildcat', 'blackfoot-cat', 'chinese-mountain-cat', 
              'domestic-cat', 'european-wildcat', 'jungle-cat', 'sand-cat']

processed_images = []

for folder in subfolders:
    folder_path = os.path.join(data_folder, folder)
    for fname in os.listdir(folder_path):
        if fname.endswith('.jpg') or fname.endswith('.png'):
            img_path = os.path.join(folder_path, fname)
            processed_images.append(load_and_preprocess_image(img_path))

images_array = np.vstack(processed_images)

print("Shape of combined image array:", images_array.shape)





def load_image(img_path, target_size=(224, 224)):
    img = tf.io.read_file(img_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, target_size)
    img = img / 255.0
    return img

def preprocess_and_save_image(img_path, target_size=(224, 224), save_dir='preprocessed_images'):
    img = load_and_preprocess_image(img_path, target_size)
    img = tf.image.convert_image_dtype(img, dtype=tf.uint8)
    img_name = os.path.basename(img_path).replace('.jpg', '.npy')
    save_path = os.path.join(save_dir, img_name)
    os.makedirs(save_dir, exist_ok=True)
    np.save(save_path, img.numpy())

def process_images_in_batches(image_paths, batch_size=4, target_size=(224, 224), save_dir='preprocessed_images'):
    for i in range(0, len(image_paths), batch_size):
        batch_paths = image_paths[i:i + batch_size]
        for path in batch_paths:
            preprocess_and_save_image(path, target_size, save_dir)
        print(f'Processed and saved batch {i // batch_size + 1}/{(len(image_paths) + batch_size - 1) // batch_size}')

data_folder = 'data/'

subfolders = ['african-wildcat', 'blackfoot-cat', 'chinese-mountain-cat', 
              'domestic-cat', 'european-wildcat', 'jungle-cat', 'sand-cat']

image_paths = []
for folder in subfolders:
    folder_path = os.path.join(data_folder, folder)
    for fname in os.listdir(folder_path):
        if fname.endswith('.jpg') or fname.endswith('.png'):
            image_paths.append(os.path.join(folder_path, fname))

process_images_in_batches(image_paths, batch_size=4)





def load_preprocessed_images(image_dir):
    image_files = [f for f in os.listdir(image_dir) if f.endswith('.npy')]
    images = [np.load(os.path.join(image_dir, f)) for f in image_files]
    return np.array(images), image_files

preprocessed_dir = 'preprocessed_images'
images, image_files = load_preprocessed_images(preprocessed_dir)

model = ResNet50(weights='imagenet')

model.summary()
























