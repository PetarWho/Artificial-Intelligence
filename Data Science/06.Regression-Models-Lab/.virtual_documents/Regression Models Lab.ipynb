import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import statsmodels.api as sm
from sklearn.datasets import make_blobs
from sklearn.preprocessing import StandardScaler











num_observations = 100 
num_features = 10

X = np.random.uniform(-10, 10, (num_observations, num_features))

true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)

noise = np.random.normal(0, 1, num_observations)  
y = X @ true_coefficients + intercept + noise

np.savez('linear_regression_data.npz', X=X, y=y)

print("Data generated and saved to 'linear_regression_data.npz'.")





data = np.load('linear_regression_data.npz')
X = data['X']
y = data['y']

plt.scatter(X[:, 0], y, alpha=0.5)
plt.xlabel('Feature 1')
plt.ylabel('Target Variable')
plt.title('Feature 1 vs Target Variable')
plt.show()


df = pd.DataFrame(X, columns=[f'Feature_{i}' for i in range(X.shape[1])])
df['Target'] = y

correlations = df.corr()
print("Correlation matrix:\n", correlations)

print("\nCorrelations with target variable:\n", correlations['Target'])


model = LinearRegression()
model.fit(X, y)

y_pred = model.predict(X)

mse = mean_squared_error(y, y_pred)
r2 = r2_score(y, y_pred)

print(f"Mean Squared Error: {mse:.2f}")
print(f"R^2 Score: {r2:.2f}")


X_with_intercept = sm.add_constant(X)

model = sm.OLS(y, X_with_intercept).fit()

print(model.summary())


residuals = y - y_pred

plt.scatter(y_pred, residuals, alpha=0.5)
plt.xlabel('Predicted Values')
plt.ylabel('Residuals')
plt.title('Residuals vs Predicted Values')
plt.axhline(y=0, color='r', linestyle='--')
plt.show()





def linear_regression_predict(X, coefficients):
    """
    Predicts the output of a linear regression model.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    coefficients (numpy.ndarray): The coefficients vector of shape (m+1,).
                                  It includes both the model coefficients and intercept.
    
    Returns:
    numpy.ndarray: The predicted values.
    """
    X_augmented = np.hstack([np.ones((X.shape[0], 1)), X])
    
    predictions = X_augmented @ coefficients
    return predictions


np.random.seed(0)  
num_observations = 100
num_features = 10
X = np.random.uniform(-10, 10, (num_observations, num_features))
true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)
y = X @ true_coefficients + intercept + np.random.normal(0, 1, num_observations)

coefficients = np.append(true_coefficients, intercept)

predictions = linear_regression_predict(X, coefficients)

print(f"First 5 predictions: {predictions[:5]}")
print(f"First 5 actual values: {y[:5]}")





def compute_cost_and_gradients(X, y, coefficients):
    """
    Compute the cost and gradients for linear regression.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    coefficients (numpy.ndarray): The coefficients vector of shape (m+1,).
                                  It includes both the model coefficients and intercept.
    
    Returns:
    cost (float): The computed cost (MSE).
    gradients (numpy.ndarray): The gradients vector of shape (m+1,).
    """
    X_augmented = np.hstack([np.ones((X.shape[0], 1)), X])
    
    predictions = X_augmented @ coefficients
    
    errors = predictions - y
    
    cost = np.mean(errors**2) / 2
    
    gradients = np.zeros(coefficients.shape)
    gradients[0] = np.mean(errors)  # Gradient w.r.t. intercept
    gradients[1:] = np.mean(errors[:, np.newaxis] * X, axis=0)  # Gradients w.r.t. coefficients
    
    return cost, gradients


np.random.seed(0) 
num_observations = 100
num_features = 10
X = np.random.uniform(-10, 10, (num_observations, num_features))
true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)
y = X @ true_coefficients + intercept + np.random.normal(0, 1, num_observations)

coefficients = np.append(true_coefficients, intercept)

cost, gradients = compute_cost_and_gradients(X, y, coefficients)

print(f"Cost: {cost:.2f}")
print(f"Gradients: {gradients}")





def gradient_descent(X, y, learning_rate=0.01, num_iterations=1000):
    """
    Perform gradient descent to fit a linear regression model.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    learning_rate (float): The learning rate for gradient descent.
    num_iterations (int): The number of iterations to perform.
    
    Returns:
    coefficients (numpy.ndarray): The optimized coefficients vector of shape (m+1,).
    costs (list): A list of cost values for each iteration.
    """
    num_observations, num_features = X.shape
    coefficients = np.zeros(num_features + 1)  # +1 for the intercept
    costs = []

    for iteration in range(num_iterations):
        X_augmented = np.hstack([np.ones((num_observations, 1)), X])
        predictions = X_augmented @ coefficients
        
        errors = predictions - y
        
        cost = np.mean(errors**2) / 2
        costs.append(cost)
        
        gradients = np.zeros(num_features + 1)
        gradients[0] = np.mean(errors)  # Gradient w.r.t. intercept
        gradients[1:] = np.mean(errors[:, np.newaxis] * X, axis=0)  # Gradients w.r.t. coefficients
        
        coefficients -= learning_rate * gradients

        if iteration % 100 == 0:
            print(f"Iteration {iteration}: Cost {cost:.4f}")

    return coefficients, costs


np.random.seed(0)
num_observations = 100
num_features = 10
X = np.random.uniform(-10, 10, (num_observations, num_features))
true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)
y = X @ true_coefficients + intercept + np.random.normal(0, 1, num_observations)

learning_rate = 0.01
num_iterations = 1000
optimized_coefficients, costs = gradient_descent(X, y, learning_rate, num_iterations)

print(f"Optimized Coefficients: {optimized_coefficients}")
print(f"Final Cost: {costs[-1]:.4f}")





def mae_cost_and_gradients(X, y, coefficients):
    """
    Compute the MAE cost and gradients for linear regression.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    coefficients (numpy.ndarray): The coefficients vector of shape (m+1,).
                                  It includes both the model coefficients and intercept.
    
    Returns:
    cost (float): The computed MAE cost.
    gradients (numpy.ndarray): The gradients vector of shape (m+1,).
    """
    X_augmented = np.hstack([np.ones((X.shape[0], 1)), X])
    
    predictions = X_augmented @ coefficients
    
    errors = predictions - y
    
    cost = np.mean(np.abs(errors))
    
    gradients = np.zeros(coefficients.shape)
    gradients[0] = np.mean(np.sign(errors)) 
    gradients[1:] = np.mean(np.sign(errors)[:, np.newaxis] * X, axis=0) 
    
    return cost, gradients


def huber_cost_and_gradients(X, y, coefficients, delta=1.0):
    """
    Compute the Huber loss cost and gradients for linear regression.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    coefficients (numpy.ndarray): The coefficients vector of shape (m+1,).
                                  It includes both the model coefficients and intercept.
    delta (float): The threshold parameter for Huber loss.
    
    Returns:
    cost (float): The computed Huber loss cost.
    gradients (numpy.ndarray): The gradients vector of shape (m+1,).
    """
    X_augmented = np.hstack([np.ones((X.shape[0], 1)), X])
    
    predictions = X_augmented @ coefficients
    
    errors = predictions - y
    
    abs_errors = np.abs(errors)
    
    huber_loss = np.where(abs_errors <= delta,
                         0.5 * errors**2,
                         delta * abs_errors - 0.5 * delta**2)
    cost = np.mean(huber_loss)
    
    gradients = np.zeros(coefficients.shape)
    gradients[0] = np.mean(np.where(abs_errors <= delta, errors, delta * np.sign(errors))) 
    gradients[1:] = np.mean(np.where(abs_errors[:, np.newaxis] <= delta, errors[:, np.newaxis] * X, delta * np.sign(errors)[:, np.newaxis] * X), axis=0)  
    
    return cost, gradients


def gradient_descent(X, y, cost_function, learning_rate=0.01, num_iterations=1000, delta=1.0):
    """
    Perform gradient descent to fit a linear regression model using a specified cost function.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    cost_function (function): The cost function to use (MAE or Huber).
    learning_rate (float): The learning rate for gradient descent.
    num_iterations (int): The number of iterations to perform.
    delta (float): The threshold parameter for Huber loss, if applicable.
    
    Returns:
    coefficients (numpy.ndarray): The optimized coefficients vector of shape (m+1,).
    costs (list): A list of cost values for each iteration.
    """
    num_observations, num_features = X.shape
    coefficients = np.zeros(num_features + 1) 
    costs = []

    for iteration in range(num_iterations):
        if cost_function == huber_cost_and_gradients:
            cost, gradients = cost_function(X, y, coefficients, delta)
        else:
            cost, gradients = cost_function(X, y, coefficients)
        
        costs.append(cost)
        
        coefficients -= learning_rate * gradients

        if iteration % 100 == 0:
            print(f"Iteration {iteration}: Cost {cost:.4f}")

    return coefficients, costs


np.random.seed(0)
num_observations = 100
num_features = 10
X = np.random.uniform(-10, 10, (num_observations, num_features))
true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)
y = X @ true_coefficients + intercept + np.random.normal(0, 1, num_observations)

print("Training with MAE...")
learning_rate = 0.01
num_iterations = 1000
optimized_coefficients_mae, costs_mae = gradient_descent(X, y, mae_cost_and_gradients, learning_rate, num_iterations)

print("Training with Huber Loss...")
delta = 1.0
optimized_coefficients_huber, costs_huber = gradient_descent(X, y, huber_cost_and_gradients, learning_rate, num_iterations, delta)

print(f"Optimized Coefficients (MAE): {optimized_coefficients_mae}")
print(f"Final Cost (MAE): {costs_mae[-1]:.4f}")
print(f"Optimized Coefficients (Huber): {optimized_coefficients_huber}")
print(f"Final Cost (Huber): {costs_huber[-1]:.4f}")








def gradient_descent(X, y, cost_function, learning_rate=0.01, num_iterations=1000, delta=1.0):
    """
    Perform gradient descent to fit a linear regression model using a specified cost function.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix of shape (n, m).
    y (numpy.ndarray): The target variable vector of shape (n,).
    cost_function (function): The cost function to use (MAE or Huber).
    learning_rate (float): The learning rate for gradient descent.
    num_iterations (int): The number of iterations to perform.
    delta (float): The threshold parameter for Huber loss, if applicable.
    
    Returns:
    coefficients (numpy.ndarray): The optimized coefficients vector of shape (m+1,).
    costs (list): A list of cost values for each iteration.
    """
    num_observations, num_features = X.shape
    coefficients = np.zeros(num_features + 1) 
    costs = []

    for iteration in range(num_iterations):
        if cost_function == huber_cost_and_gradients:
            cost, gradients = cost_function(X, y, coefficients, delta)
        else:
            cost, gradients = cost_function(X, y, coefficients)
        
        costs.append(cost)
        
        coefficients -= learning_rate * gradients

        # Print progress
        if iteration % 100 == 0:
            print(f"Iteration {iteration}: Cost {cost:.4f}")

    return coefficients, costs

np.random.seed(0)  
num_observations = 100
num_features = 10
X = np.random.uniform(-10, 10, (num_observations, num_features))
true_coefficients = np.random.uniform(-5, 5, num_features)
intercept = np.random.uniform(-10, 10)
y = X @ true_coefficients + intercept + np.random.normal(0, 1, num_observations)

learning_rates = [1e-6, 1e-2, 1e0]
num_iterations = 1000
delta = 1.0

for lr in learning_rates:
    print(f"Training with Learning Rate: {lr}")
    optimized_coefficients, costs = gradient_descent(X, y, huber_cost_and_gradients, lr, num_iterations, delta)
    print(f"Final Cost (Learning Rate {lr}): {costs[-1]:.4f}\n")





def generate_classification_data(n_samples=300, n_features=10, n_clusters=2, random_state=0):
    """
    Generate synthetic classification data using make_blobs.
    
    Parameters:
    n_samples (int): Number of samples to generate.
    n_features (int): Number of features for each sample.
    n_clusters (int): Number of clusters (classes) to generate.
    random_state (int): Seed for the random number generator.
    
    Returns:
    X (numpy.ndarray): The input feature matrix of shape (n_samples, n_features).
    y (numpy.ndarray): The target variable vector of shape (n_samples,).
    """
    X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=random_state)
    
    return X, y

def plot_classification_data(X, y):
    """
    Plot the synthetic classification data.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix.
    y (numpy.ndarray): The target variable vector.
    """
    plt.figure(figsize=(10, 6))
    scatter = plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)
    plt.colorbar(scatter, label='Class')
    plt.xlabel('Feature 1')
    plt.ylabel('Feature 2')
    plt.title('Synthetic Classification Data')
    plt.show()


n_samples = 300
n_features = 10
n_clusters = 2
X, y = generate_classification_data(n_samples, n_features, n_clusters)

if n_features >= 2:
    plot_classification_data(X, y)
else:
    print(f"Data generated with {n_samples} samples and {n_features} features.")





def sigmoid(z):
    """
    Compute the sigmoid of z.
    
    Parameters:
    z (numpy.ndarray): The input array.
    
    Returns:
    numpy.ndarray: The sigmoid of the input array.
    """
    return 1 / (1 + np.exp(-z))


def logistic_model(X, coefficients):
    """
    Compute the predictions for logistic regression.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix.
    coefficients (numpy.ndarray): The model coefficients including the intercept.
    
    Returns:
    numpy.ndarray: The predicted probabilities.
    """
    z = np.dot(X, coefficients[1:]) + coefficients[0]
    return sigmoid(z)


def cross_entropy_cost_and_gradients(X, y, coefficients):
    """
    Compute the cross-entropy cost and gradients for logistic regression.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix.
    y (numpy.ndarray): The true labels.
    coefficients (numpy.ndarray): The model coefficients including the intercept.
    
    Returns:
    tuple: (cost, gradients)
        cost (float): The cross-entropy cost.
        gradients (numpy.ndarray): The gradients of the cost with respect to the coefficients.
    """
    m = len(y)
    predictions = logistic_model(X, coefficients)
    
    predictions = np.clip(predictions, 1e-10, 1 - 1e-10)
    
    cost = -np.mean(y * np.log(predictions) + (1 - y) * np.log(1 - predictions))
    
    errors = predictions - y
    gradient_intercept = np.mean(errors)
    gradient_features = np.dot(X.T, errors) / m
    
    gradients = np.concatenate(([gradient_intercept], gradient_features))
    
    return cost, gradients

def gradient_descent(X, y, cost_function, learning_rate=0.01, num_iterations=1000):
    """
    Perform gradient descent to fit a logistic regression model.
    
    Parameters:
    X (numpy.ndarray): The input feature matrix.
    y (numpy.ndarray): The target variable vector.
    cost_function (function): The cost function to use (cross-entropy).
    learning_rate (float): The learning rate for gradient descent.
    num_iterations (int): The number of iterations to perform.
    
    Returns:
    tuple: (optimized_coefficients, costs)
        optimized_coefficients (numpy.ndarray): The optimized coefficients vector.
        costs (list): A list of cost values for each iteration.
    """
    num_observations, num_features = X.shape
    coefficients = np.zeros(num_features + 1)  # +1 for the intercept
    costs = []

    for iteration in range(num_iterations):
        cost, gradients = cost_function(X, y, coefficients)
        costs.append(cost)
        
        coefficients -= learning_rate * gradients

        if iteration % 100 == 0:
            print(f"Iteration {iteration}: Cost {cost:.4f}")

    return coefficients, costs


np.random.seed(0)
n_samples = 300
n_features = 10
n_clusters = 2
X, y = generate_classification_data(n_samples, n_features, n_clusters)

scaler = StandardScaler()
X = scaler.fit_transform(X)

learning_rates = [1e-6, 1e-3, 1e-1]
num_iterations = 1000

for lr in learning_rates:
    print(f"Training with Learning Rate: {lr}")
    optimized_coefficients, costs = gradient_descent(X, y, cross_entropy_cost_and_gradients, lr, num_iterations)
    print(f"Final Cost (Learning Rate {lr}): {costs[-1]:.4f}\n")






