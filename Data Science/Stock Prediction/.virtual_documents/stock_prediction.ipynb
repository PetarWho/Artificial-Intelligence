








import os
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import pandas as pd
import datetime as dt

from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, load_model
from tensorflow.keras.layers import Dense, Dropout, LSTM
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras import Input











def read_and_normalize_csv(csv_path):
    df = pd.read_csv(csv_path) # Read csv file
    df.columns = df.columns.str.lower() # Turn names into lowercase
    df.columns = df.columns.str.replace(' ', '_') # Replace spaces with underscores
    df['date'] = pd.to_datetime(df['date']) # Turn date column into datetime
    return df








def split_datasets_by_date(dataframe, date):
    data = dataframe[dataframe['date'] < date]
    test = dataframe[dataframe['date'] >= date]
    return data, test








def scale_data(data, feature_range=(0,1)):
    scaler = MinMaxScaler(feature_range=feature_range)
    scaled_data = scaler.fit_transform(data)
    return scaled_data, scaler








def prepare_training_data(scaled_data, prediction_days):
    x_train, y_train = [], []

    for x in range(prediction_days, len(scaled_data)):
        x_train.append(scaled_data[x-prediction_days:x, 0])
        y_train.append(scaled_data[x, 0])

    x_train, y_train = np.array(x_train), np.array(y_train)
    x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))

    return x_train, y_train








def build_lstm_model(input_shape, units=50, dropout_rate=0.2):
    model = Sequential()

    # Define the input layer
    model.add(Input(shape=input_shape))

    # Add LSTM layers
    model.add(LSTM(units=units, return_sequences=True))
    model.add(Dropout(dropout_rate))

    model.add(LSTM(units=units, return_sequences=True))
    model.add(Dropout(dropout_rate))

    model.add(LSTM(units=units))
    model.add(Dropout(dropout_rate))

    # Add the output layer
    model.add(Dense(units=1))  # Prediction of the next closing value

    model.compile(optimizer='adam', loss='mean_squared_error')
    return model








def prepare_test_data(dataframe, test_data, prediction_days, scaler):
    total_dataset = pd.concat((dataframe['close'], test_data['close']), axis=0)
    model_inputs = total_dataset[len(total_dataset) - len(test_data) - prediction_days:].values
    model_inputs = model_inputs.reshape(-1, 1)
    model_inputs = scaler.transform(model_inputs)

    x_test = []

    for x in range(prediction_days, len(model_inputs)):
        x_test.append(model_inputs[x-prediction_days:x, 0])

    x_test = np.array(x_test)
    x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))

    return x_test








def plot_predictions(actual_prices, predicted_prices, stock_ticker_name, test_dates):
    plt.figure(figsize=(10, 6))
    
    # Plot actual prices
    plt.plot(test_dates, actual_prices, color='black', label=f'Actual {stock_ticker_name} Price')
    
    # Plot predicted prices
    plt.plot(test_dates, predicted_prices, color='green', label=f'Predicted {stock_ticker_name} Price')
    
    # Formatting the plot
    plt.title(f'{stock_ticker_name} Share Price')
    plt.xlabel('Date')
    plt.ylabel(f'{stock_ticker_name} Share Price')
    
    # Rotate and format date labels
    plt.xticks(rotation=45)
    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))
    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=1))  # Adjust this as needed
    
    plt.legend()
    plt.tight_layout()
    plt.show()








def predict_future_prices(model, model_inputs, prediction_days, scaler):
    real_data = [model_inputs[len(model_inputs) + 1 - prediction_days:len(model_inputs + 1), 0]]
    real_data = np.array(real_data)
    real_data = np.reshape(real_data, (real_data.shape[0], real_data.shape[1], 1))

    prediction = model.predict(real_data)
    prediction = scaler.inverse_transform(prediction)
    return prediction








def save_model(model, file_path):
    model.save(file_path)








def load_saved_model(file_path):
    return load_model(file_path)








stock_ticker_name = 'TWTR' # Ticker name of the stock (if you use TWTR you should have csv file called twtr-stocks)
split_date = '2020-01-01' # Date that will split the dataset into train and test datasets (before that date - train, after - test)
epochs = 0 # Epochs of training (use 0 if you want to just load an existing model and not train it any further)
batch_size = 256 # Batch Size - The number of data samples used in each iteration of training
prediction_days = 30 # The number of previous days used to predict the next day (sequence length).
early_stop_patience = 6 # The number of bad results needed to stop the model from training
csv_path = f'{stock_ticker_name.lower()}-stocks.csv'
model_save_path = f'{stock_ticker_name}_saved_model.keras'

# Step 1: Read and normalize the data
df = read_and_normalize_csv(csv_path)

# Step 2: Split the datasets
data, test = split_datasets_by_date(df, split_date)

# Step 3: Scale the data
scaled_data, scaler = scale_data(data['close'].values.reshape(-1, 1))

# Step 4: Prepare the training data
x_train, y_train = prepare_training_data(scaled_data, prediction_days)

# Step 5: Build and train the model or load it if we already have it saved
if epochs:
    # Implementing early stopping to prevent performance decrease
    early_stopping = EarlyStopping(monitor='loss', patience=early_stop_patience, restore_best_weights=True)
    if os.path.exists(model_save_path):
        print(f"Loading model from {model_save_path}...")
        model = load_saved_model(model_save_path)
        
        print(f"Continuing training for {epochs} more epochs...")
        model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])
        
        # Save the model after additional training
        # save_model(model, model_save_path)
        # print(f"Existing model saved to {model_save_path} after additional training.")
    else:
        print("Model not found. Training a new model...")
        
        # Step 5: Build the model 
        model = build_lstm_model(input_shape=(x_train.shape[1], 1))
        
        # Train the model
        model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])
        
        # Save the model after training
        save_model(model, model_save_path)
        print(f"Model saved to {model_save_path}")
else:
    print(f"Loading model from {model_save_path}...")
    model = load_saved_model(model_save_path)

# Step 6: Prepare the test data
x_test = prepare_test_data(df, test, prediction_days, scaler)

# Step 7: Make predictions
predicted_prices = model.predict(x_test)
predicted_prices = scaler.inverse_transform(predicted_prices)

# Step 8: Plot the results
test_dates = test['date'].values
plot_predictions(test['close'].values, predicted_prices, stock_ticker_name, test_dates)

# Step 9: Predict future prices
future_prediction = predict_future_prices(model, model_inputs=x_test, prediction_days=prediction_days, scaler=scaler)
print(f"Predicted price for the next day: {future_prediction}")





#save_model(model, "test.keras")



